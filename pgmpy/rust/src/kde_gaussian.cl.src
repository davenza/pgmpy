// Row major indexing for row i column j and leading dimension for the columns
#define RM(i, j, leading) ((i)*(leading) + (j))
// Column major indexing for row i column j and leading dimension for the rows
#define CM(i, j, leading) ((j)*(leading) + (i))

#define BASE_RM(i, j, leading) ((i)*(leading))
#define BASE_CM(i, j, leading) (i)

#define ADD_BASE_RM(i, j, leading) (j)
#define ADD_BASE_CM(i, j, leading) ((j)*(leading))

/**
##########################################
################  MISC  ##################
##########################################
*/


__kernel void fill_value(__global double *vec, __private double value) {
    vec[get_global_id(0)] = value;
}

__kernel void fill_value_uint(__global uint *vec, __private uint value) {
    vec[get_global_id(0)] = value;
}

__kernel void sum_vectors(__global double *left, __constant double *right) {
    uint idx = get_global_id(0);
    left[idx] += right[idx];
}

__kernel void sum_constant(__global double *v, __private double c) {
    v[get_global_id(0)] += c;
}


/**
##########################################
###############  COMMON  #################
##########################################
 */

/**begin repeat
 * #train_mode = rowmajor, columnmajor#
 * #train_indexer = RM, CM#
 * #find_column = %, /#
 * #find_row = /, %#
 */

/**begin repeat1
 * #test_mode = rowmajor, columnmajor#
 * #test_indexer = RM, CM#
 */

__kernel void substract_@train_mode@_@test_mode@(__constant double *train_data,
                                            __private uint train_cols,
                                            __constant double *vec,
                                            __global double *res,
                                            __private uint row,
                                            __private uint train_leading_dimension,
                                            __private uint test_leading_dimension)
{
    int i = get_global_id(0);

    int r = i @find_row@ train_leading_dimension;
    int c = i @find_column@ train_leading_dimension;

    res[RM(r, c, train_cols)] = train_data[i] - vec[@test_indexer@(row, c, test_leading_dimension)];
}

/**end repeat1**/

/**end repeat**/

__kernel void solve(__global double *diff_data, __constant double *chol, __private uint n_col) {
    uint r = get_global_id(0);
    uint index_row = r * n_col;

    for (uint c = 0; c < n_col; c++) {
        for (uint i = 0; i < c; i++) {
            diff_data[index_row + c] -= chol[c * n_col + i] * diff_data[index_row + i];
        }
        diff_data[index_row + c] /= chol[c * n_col + c];
    }
}

__kernel void square(__global double *solve_data) {
    uint idx = get_global_id(0);
    double d = solve_data[idx];
    solve_data[idx] = d * d;
}

/**
##########################################
#################  PDF  ##################
##########################################
*/

__kernel void sumout(__constant double *square_data,
                    __global double *sol_vec,
                    __private uint n_col,
                    __private double lognorm_factor) {
    uint r = get_global_id(0);
    uint idx = r * n_col;

    sol_vec[r] = square_data[idx];
    for (uint i = 1; i < n_col; i++) {
        sol_vec[r] += square_data[idx + i];
    }

    sol_vec[r] = exp(-0.5 * sol_vec[r] - lognorm_factor);
}

__kernel void sum_gpu_vec(__global double *input,
                          __local double *localSums) {
    uint global_id = get_global_id(0);
    uint local_id = get_local_id(0);
    uint group_size = get_local_size(0);
    uint group_id = get_group_id(0);
    uint num_groups = get_num_groups(0);

    if (group_id == num_groups) {
        group_size = get_global_size(0) - group_id*group_size;
    }

    localSums[local_id] = input[global_id];

    while (group_size > 1) {
        int stride = group_size / 2;
        barrier(CLK_LOCAL_MEM_FENCE);
        if (group_size % 2 == 0) {
            if (local_id < stride) {
                localSums[local_id] += localSums[local_id + stride];
            }

            group_size = group_size / 2;
        }
        else {
            if (local_id < stride) {
                localSums[local_id+1] += localSums[local_id+1 + stride];
            }
            group_size = (group_size / 2) + 1;
        }
    }

    barrier(CLK_LOCAL_MEM_FENCE);
    if (local_id == 0) {
        input[group_id] = localSums[0];
    }
}

/**
##########################################
########  logPDF - Iterate test  #########
##########################################
*/

__kernel void logsumout(__constant double *square_data,
                        __global double *sol_vec,
                        __private uint n_col,
                        __private double lognorm_factor) {
    uint r = get_global_id(0);
    uint idx = r * n_col;

    sol_vec[r] = square_data[idx];
    for (uint i = 1; i < n_col; i++) {
        sol_vec[r] += square_data[idx + i];
    }

    sol_vec[r] = (-0.5 * sol_vec[r]) - lognorm_factor;
}

__kernel void max_gpu_vec_copy(__constant double *input,
                               __global double *maxGroups,
                               __local double *localMaxs) {
    uint global_id = get_global_id(0);
    uint local_id = get_local_id(0);
    uint group_size = get_local_size(0);
    uint group_id = get_group_id(0);
    uint num_groups = get_num_groups(0);

    if (group_id == num_groups) {
        group_size = get_global_size(0) - group_id*group_size;
    }

    localMaxs[local_id] = input[global_id];

    while (group_size > 1) {
        int stride = group_size / 2;
        barrier(CLK_LOCAL_MEM_FENCE);
        if (group_size % 2 == 0) {
            if (local_id < stride) {
                localMaxs[local_id] = max(localMaxs[local_id], localMaxs[local_id + stride]);
            }

            group_size = group_size / 2;
        }
        else {
            if (local_id < stride) {
                localMaxs[local_id+1] = max(localMaxs[local_id+1 + stride], localMaxs[local_id+1]);
            }
            group_size = (group_size / 2) + 1;
        }
    }

    barrier(CLK_GLOBAL_MEM_FENCE);
    if (local_id == 0) {
        maxGroups[group_id] = localMaxs[0];
    }
}

__kernel void max_gpu_vec(__global double* maxGroups,
                     __local double *localMaxs) {

    uint global_id = get_global_id(0);
    uint local_id = get_local_id(0);
    uint group_size = get_local_size(0);
    uint group_id = get_group_id(0);
    uint num_groups = get_num_groups(0);

    if (group_id == num_groups) {
        group_size = get_global_size(0) - group_id*group_size;
    }

    localMaxs[local_id] = maxGroups[global_id];

    while (group_size > 1) {
        int stride = group_size / 2;
        barrier(CLK_LOCAL_MEM_FENCE);
        if (group_size % 2 == 0) {
            if (local_id < stride) {
                localMaxs[local_id] = max(localMaxs[local_id], localMaxs[local_id + stride]);
            }

            group_size = group_size / 2;
        }
        else {
            if (local_id < stride) {
                localMaxs[local_id+1] = max(localMaxs[local_id+1 + stride], localMaxs[local_id+1]);
            }
            group_size = (group_size / 2) + 1;
        }
    }

    barrier(CLK_GLOBAL_MEM_FENCE);
    if (local_id == 0) {
        maxGroups[group_id] = localMaxs[0];
    }
}

__kernel void log_sum_gpu_vec(__global double *input,
                          __local double *localSums,
                          __constant double *maxexp) {
    uint global_id = get_global_id(0);
    uint local_id = get_local_id(0);
    uint group_size = get_local_size(0);
    uint group_id = get_group_id(0);
    uint num_groups = get_num_groups(0);

    if (group_id == num_groups) {
        group_size = get_global_size(0) - group_id*group_size;
    }

    localSums[local_id] = exp(input[global_id]-maxexp[0]);

    while (group_size > 1) {
        int stride = group_size / 2;
        barrier(CLK_LOCAL_MEM_FENCE);
        if (group_size % 2 == 0) {
            if (local_id < stride) {
                localSums[local_id] += localSums[local_id + stride];
            }

            group_size = group_size / 2;
        }
        else {
            if (local_id < stride) {
                localSums[local_id+1] += localSums[local_id+1 + stride];
            }
            group_size = (group_size / 2) + 1;
        }
    }

    barrier(CLK_LOCAL_MEM_FENCE);
    if (local_id == 0) {
        input[group_id] = localSums[0];
    }
}

__kernel void copy_logpdf_result(__constant double *logsum,
                                 __constant double *maxexp,
                                 __global double *res,
                                 __private uint res_offset) {
    res[res_offset] = maxexp[0] + log(logsum[0]);
}


/**
##########################################
## logPDF - Iterate train (low memory) ###
##########################################
*/

__kernel void logsumout_checkmax(__constant double *square_data,
                                __global double *sol_vec,
                                __global double *max_vec,
                                __private uint n_col,
                                __private double lognorm_factor) {
    uint r = get_global_id(0);
    uint idx = r * n_col;

    sol_vec[r] = square_data[idx];
    for (uint i = 1; i < n_col; i++) {
        sol_vec[r] += square_data[idx + i];
    }

    sol_vec[r] = (-0.5 * sol_vec[r]) - lognorm_factor;
    max_vec[r] = max(max_vec[r], sol_vec[r]);
}

__kernel void exp_and_sum(__constant double* logsum, __constant double* maxexp, __global double *res) {
    uint idx = get_global_id(0);
    res[idx] += exp(logsum[idx] - maxexp[idx]);
}

__kernel void log_and_sum(__global double* res, __constant double* maxexp) {
    uint idx = get_global_id(0);
    res[idx] = log(res[idx]) + maxexp[idx];
}



/**
##########################################
## logPDF - Iterate train (high memory) ##
##########################################
*/

__kernel void logsumout_to_matrix(__constant double *square_data,
                                    __global double *sol_mat,
                                    __private uint n_col,
                                    __private uint sol_row,
                                    __private uint n_train_instances,
                                    __private double lognorm_factor) {
    uint r = n_train_instances*get_global_id(0) + sol_row;
    uint idx = get_global_id(0) * n_col;

    sol_mat[r] = square_data[idx];
    for (uint i = 1; i < n_col; i++) {
        sol_mat[r] += square_data[idx + i];
    }

    sol_mat[r] = (-0.5 * sol_mat[r]) - lognorm_factor;
}

__kernel void max_gpu_mat_copy(__constant double *input,
                               __global double* maxGroups,
                               __local double *localMaxs,
                               __private uint array_n_cols) {

    uint global_id_row = get_global_id(0);
    uint global_id_col = get_global_id(1);
    uint n_cols = get_global_size(1);
    uint local_id = get_local_id(1);
    uint group_size = get_local_size(1);
    uint group_id = get_group_id(1);
//   FIXME: This code returns num_groups = 3 for global_size = 1000 and local_size = 256, so it does not work as expected
//      when local_work_size does not evenly divide global_work_size.
//    uint num_groups = get_num_groups(1);

    //This is equal to ceil(n_cols/group_size): https://stackoverflow.com/questions/2745074/fast-ceiling-of-an-integer-division-in-c-c
    uint num_groups = (n_cols + group_size - 1) / group_size;

    if (group_id+1 == num_groups) {
        group_size = get_global_size(1) - group_id*group_size;
    }

    localMaxs[local_id] = input[global_id_row*array_n_cols + global_id_col];

    while (group_size > 1) {
        int stride = group_size / 2;
        barrier(CLK_LOCAL_MEM_FENCE);
        if (group_size % 2 == 0) {
            if (local_id < stride) {
                localMaxs[local_id] = max(localMaxs[local_id], localMaxs[local_id+stride]);
            }
            group_size = group_size / 2;
        }
        else {
            if (local_id < stride) {
                localMaxs[local_id+1] = max(localMaxs[local_id+1], localMaxs[local_id+1+stride]);
            }
            group_size = (group_size / 2) + 1;
        }
    }

    barrier(CLK_GLOBAL_MEM_FENCE);
    if (local_id == 0) {
        maxGroups[global_id_row*num_groups+group_id] = localMaxs[0];
    }
}

__kernel void max_gpu_mat(__global double* maxGroups,
                          __local double *localMaxs,
                          __private uint array_n_cols) {

    uint global_id_row = get_global_id(0);
    uint global_id_col = get_global_id(1);
    uint n_cols = get_global_size(1);
    uint local_id = get_local_id(1);
    uint group_size = get_local_size(1);
    uint group_id = get_group_id(1);
//   FIXME: This code returns num_groups = 3 for global_size = 1000 and local_size = 256, so it does not work as expected
//      when local_work_size does not evenly divide global_work_size.
//    uint num_groups = get_num_groups(1);

    //This is equal to ceil(n_cols/group_size): https://stackoverflow.com/questions/2745074/fast-ceiling-of-an-integer-division-in-c-c
    uint num_groups = (n_cols + group_size - 1) / group_size;

    if (group_id+1 == num_groups) {
        group_size = get_global_size(1) - group_id*group_size;
    }

    localMaxs[local_id] = maxGroups[global_id_row*array_n_cols + global_id_col];

    while (group_size > 1) {
        int stride = group_size / 2;
        barrier(CLK_LOCAL_MEM_FENCE);
        if (group_size % 2 == 0) {
            if (local_id < stride) {
                localMaxs[local_id] = max(localMaxs[local_id], localMaxs[local_id + stride]);
            }

            group_size = group_size / 2;
        }
        else {
            if (local_id < stride) {
                localMaxs[local_id+1] = max(localMaxs[local_id+1 + stride], localMaxs[local_id+1]);
            }
            group_size = (group_size / 2) + 1;
        }
    }

    barrier(CLK_GLOBAL_MEM_FENCE);
    if (local_id == 0) {
        maxGroups[global_id_row*array_n_cols + group_id] = localMaxs[0];
    }
}


__kernel void exp_and_sum_mat(__global double* res, __constant double* maxexp, __private uint n, __private uint num_groups) {
    uint i = get_global_id(0);
    uint row = i / n;
    uint col = i % n;
    uint d = RM(row, col, n);
    res[d] = exp(res[d] - maxexp[row*num_groups]);
}

__kernel void sum_gpu_mat(__global double* maxGroups,
                          __local double *localMaxs,
                          __private uint array_n_cols) {

    uint global_id_row = get_global_id(0);
    uint global_id_col = get_global_id(1);
    uint n_cols = get_global_size(1);
    uint local_id = get_local_id(1);
    uint group_size = get_local_size(1);
    uint group_id = get_group_id(1);
//   FIXME: This code returns num_groups = 3 for global_size = 1000 and local_size = 256, so it does not work as expected
//      when local_work_size does not evenly divide global_work_size.
//    uint num_groups = get_num_groups(1);

    //This is equal to ceil(n_cols/group_size): https://stackoverflow.com/questions/2745074/fast-ceiling-of-an-integer-division-in-c-c
    uint num_groups = (n_cols + group_size - 1) / group_size;

    if (group_id+1 == num_groups) {
        group_size = get_global_size(1) - group_id*group_size;
    }

    localMaxs[local_id] = maxGroups[global_id_row*array_n_cols + global_id_col];

    while (group_size > 1) {
        int stride = group_size / 2;
        barrier(CLK_LOCAL_MEM_FENCE);
        if (group_size % 2 == 0) {
            if (local_id < stride) {
                localMaxs[local_id] += localMaxs[local_id+stride];
            }

            group_size = group_size / 2;
        }
        else {
            if (local_id < stride) {
                localMaxs[local_id+1] += localMaxs[local_id+stride+1];
            }
            group_size = (group_size / 2) + 1;
        }
    }

    barrier(CLK_GLOBAL_MEM_FENCE);
    if (local_id == 0) {
        maxGroups[global_id_row*array_n_cols + group_id] = localMaxs[0];
    }
}

__kernel void log_and_sum_mat(__global double* res,
                                __constant double *summed_mat,
                                __constant double* maxexp,
                                __private uint n_col,
                                __private uint num_groups) {
    uint idx = get_global_id(0);
    res[idx] = log(summed_mat[idx*n_col]) + maxexp[idx*num_groups];
}

/**
##########################################
########## Denominator Only KDE ##########
##########################################
*/

/**begin repeat
 * #train_mode = rowmajor, columnmajor#
 * #train_indexer = RM, CM#
 */

/**begin repeat1
 * #test_mode = rowmajor, columnmajor#
 * #test_indexer = RM, CM#
 */

__kernel void substract_without_origin_@train_mode@_@test_mode@(__constant double *train_data,
                                                                __private uint train_leading_dimension,
                                                                __constant double *test_data,
                                                                __private uint test_leading_dimension,
                                                                __global double *res,
                                                                __private uint test_row,
                                                                __private uint n_cols) {

    int gid = get_global_id(0);

    int r = gid / n_cols;
    int c = gid % n_cols;

    res[RM(r, c, n_cols)] = test_data[@test_indexer@(test_row, c+1, test_leading_dimension)]
                        - train_data[@train_indexer@(r, c+1, train_leading_dimension)];
}

/**end repeat1**/

/**end repeat**/

__kernel void precompute_marginal_precision(__constant double* precision,
                                                        __private double inv_precision_variable,
                                                        __private uint d,
                                                        __global double* res) {
    uint gid = get_global_id(0);

    uint r = gid / (d-1);
    uint c = gid % (d-1);


    res[RM(r,c,d-1)] = precision[r+1]*precision[c+1]*inv_precision_variable - precision[RM(r+1,c+1,d)];
}

__kernel void onlykde_exponent_coefficients_iterate_test(__global double* Ti,
        __private uint nparents_kde,
        __global double* marginal_precision,
        __global double* train_coefficients,
        __local double* sums_buffer
)
{
    uint gid = get_global_id(0);
    uint lid = get_local_id(0);

    uint instance = get_group_id(0);

    uint p = lid / nparents_kde;
    uint q = lid % nparents_kde;

    uint base_pos = BASE_RM(instance, 0, nparents_kde);

    double Tp = Ti[base_pos + ADD_BASE_RM(instance, p, nparents_kde)];
    double Tq = Ti[base_pos + ADD_BASE_RM(instance, q, nparents_kde)];


    sums_buffer[lid] = Tq*marginal_precision[RM(q,p,nparents_kde)];


    uint remaining_sum = nparents_kde;

    if (q < remaining_sum / 2) {
        while (remaining_sum > 1) {
            uint stride = remaining_sum / 2;
            barrier(CLK_LOCAL_MEM_FENCE);

            if (q < stride) {
                sums_buffer[lid] += sums_buffer[lid+stride];
            }

            if (remaining_sum % 2 != 0 && q == 0) {
                sums_buffer[lid] += sums_buffer[lid+remaining_sum-1];
            }

            remaining_sum = stride;
        }
    }

    barrier(CLK_LOCAL_MEM_FENCE);
    if (q == 0) {
        sums_buffer[p] = sums_buffer[lid] * Tp;
    }

    remaining_sum = nparents_kde;

    if (lid < remaining_sum / 2) {
        while (remaining_sum > 1) {
            uint stride = remaining_sum / 2;
            barrier(CLK_LOCAL_MEM_FENCE);

            if (lid < stride) {
                sums_buffer[lid] += sums_buffer[lid+stride];
            }

            if (remaining_sum % 2 != 0 && lid == 0) {
                sums_buffer[lid] += sums_buffer[lid+remaining_sum-1];
            }

            remaining_sum = stride;
        }
    }

    barrier(CLK_LOCAL_MEM_FENCE);
    if (lid == 0) {
        train_coefficients[instance] = 0.5*sums_buffer[0];
    }
}

__kernel void onlykde_exponent_coefficients_iterate_train_high_memory(__global double* Ti,
        __private uint nparents_kde,
        __global double* marginal_precision,
        __global double* train_coefficients,
        __local double* sums_buffer,
        __private uint train_index,
        __private uint n
)
{
    uint gid = get_global_id(0);
    uint lid = get_local_id(0);

    uint instance = get_group_id(0);

    uint p = lid / nparents_kde;
    uint q = lid % nparents_kde;

    uint base_pos = BASE_RM(instance, 0, nparents_kde);

    double Tp = Ti[base_pos + ADD_BASE_RM(instance, p, nparents_kde)];
    double Tq = Ti[base_pos + ADD_BASE_RM(instance, q, nparents_kde)];

    sums_buffer[lid] = Tq*marginal_precision[RM(q,p,nparents_kde)];

    uint remaining_sum = nparents_kde;

    if (q < remaining_sum / 2) {
        while (remaining_sum > 1) {
            uint stride = remaining_sum / 2;
            barrier(CLK_LOCAL_MEM_FENCE);

            if (q < stride) {
                sums_buffer[lid] += sums_buffer[lid+stride];
            }

            if (remaining_sum % 2 != 0 && q == 0) {
                sums_buffer[lid] += sums_buffer[lid+remaining_sum-1];
            }

            remaining_sum = stride;
        }
    }

    barrier(CLK_LOCAL_MEM_FENCE);
    if (q == 0) {
        sums_buffer[p] = sums_buffer[lid] * Tp;
    }

    remaining_sum = nparents_kde;

    if (lid < remaining_sum / 2) {
        while (remaining_sum > 1) {
            uint stride = remaining_sum / 2;
            barrier(CLK_LOCAL_MEM_FENCE);

            if (lid < stride) {
                sums_buffer[lid] += sums_buffer[lid+stride];
            }

            if (remaining_sum % 2 != 0 && lid == 0) {
                sums_buffer[lid] += sums_buffer[lid+remaining_sum-1];
            }

            remaining_sum = stride;
        }
    }

    barrier(CLK_LOCAL_MEM_FENCE);
    if (lid == 0) {
        train_coefficients[RM(instance, train_index, n)] = 0.5*sums_buffer[0];
    }
}

__kernel void onlykde_exponent_coefficients_iterate_train_low_memory_checkmax(__global double* Ti,
        __private uint nparents_kde,
        __global double* marginal_precision,
        __global double* max_buffer,
        __local double* sums_buffer,
        __private uint n
)
{

    uint gid = get_global_id(0);
    uint lid = get_local_id(0);

    uint instance = get_group_id(0);

    uint p = lid / nparents_kde;
    uint q = lid % nparents_kde;

    uint base_pos = BASE_RM(instance, 0, nparents_kde);

    double Tp = Ti[base_pos + ADD_BASE_RM(instance, p, nparents_kde)];
    double Tq = Ti[base_pos + ADD_BASE_RM(instance, q, nparents_kde)];

    sums_buffer[lid] = Tq*marginal_precision[RM(q,p,nparents_kde)];

    uint remaining_sum = nparents_kde;

    if (q < remaining_sum / 2) {
        while (remaining_sum > 1) {
            uint stride = remaining_sum / 2;
            barrier(CLK_LOCAL_MEM_FENCE);

            if (q < stride) {
                sums_buffer[lid] += sums_buffer[lid+stride];
            }

            if (remaining_sum % 2 != 0 && q == 0) {
                sums_buffer[lid] += sums_buffer[lid+remaining_sum-1];
            }

            remaining_sum = stride;
        }
    }

    barrier(CLK_LOCAL_MEM_FENCE);
    if (q == 0) {
        sums_buffer[p] = sums_buffer[lid] * Tp;
    }

    remaining_sum = nparents_kde;

    if (lid < remaining_sum / 2) {
        while (remaining_sum > 1) {
            uint stride = remaining_sum / 2;
            barrier(CLK_LOCAL_MEM_FENCE);

            if (lid < stride) {
                sums_buffer[lid] += sums_buffer[lid+stride];
            }

            if (remaining_sum % 2 != 0 && lid == 0) {
                sums_buffer[lid] += sums_buffer[lid+remaining_sum-1];
            }

            remaining_sum = stride;
        }
    }

    barrier(CLK_LOCAL_MEM_FENCE);
    if (lid == 0) {
        max_buffer[instance] = max(max_buffer[instance], 0.5*sums_buffer[0]);
    }
}

__kernel void onlykde_exponent_coefficients_iterate_train_low_memory_compute(__global double* Ti,
        __private uint nparents_kde,
        __global double* marginal_precision,
        __global double* final_result,
        __global double* max_buffer,
        __local double* sums_buffer,
        __private uint n
)
{

    uint gid = get_global_id(0);
    uint lid = get_local_id(0);

    uint instance = get_group_id(0);

    uint p = lid / nparents_kde;
    uint q = lid % nparents_kde;

    uint base_pos = BASE_RM(instance, 0, nparents_kde);

    double Tp = Ti[base_pos + ADD_BASE_RM(instance, p, nparents_kde)];
    double Tq = Ti[base_pos + ADD_BASE_RM(instance, q, nparents_kde)];

    sums_buffer[lid] = Tq*marginal_precision[RM(q,p,nparents_kde)];

    uint remaining_sum = nparents_kde;

    while (remaining_sum > 1) {
        uint stride = remaining_sum / 2;
        barrier(CLK_LOCAL_MEM_FENCE);

        if (q < stride) {
            sums_buffer[lid] += sums_buffer[lid+stride];
        }

        if (remaining_sum % 2 != 0 && q == 0) {
            sums_buffer[lid] += sums_buffer[lid+remaining_sum-1];
        }

        remaining_sum = stride;
    }

    barrier(CLK_LOCAL_MEM_FENCE);
    if (q == 0) {
        sums_buffer[p] = sums_buffer[lid] * Tp;
    }

    remaining_sum = nparents_kde;

    while (remaining_sum > 1) {
        uint stride = remaining_sum / 2;
        barrier(CLK_LOCAL_MEM_FENCE);

        if (lid<stride) {
            sums_buffer[lid] += sums_buffer[lid+stride];
        }

        if (remaining_sum % 2 != 0 && lid == 0) {
            sums_buffer[lid] += sums_buffer[lid+remaining_sum-1];
        }

        remaining_sum = stride;
    }

    barrier(CLK_LOCAL_MEM_FENCE);
    if (lid == 0) {
        final_result[instance] += exp(0.5*sums_buffer[0] - max_buffer[instance]);
    }
}

/**
##########################################
####### Denominator Only Gaussian ########
##########################################
*/

/**begin repeat
 * #mode = rowmajor, columnmajor#
 * #indexer = RM, CM#
 */

__kernel void s1_and_s3_sum_parents_@mode@(__constant double* test_dataset,
                                        __private uint leading_dimension,
                                        __constant double* beta,
                                        __private uint variable_index,
                                        __constant uint* evidence_index,
                                        __private uint len_evidence,
                                        __private double inv_variance,
                                        __global double* s1,
                                        __global double* s3
)
{
    uint row = get_global_id(0);
    double Cj = beta[0];

    uint base_pos = BASE_@indexer@(row, 0, leading_dimension);
    for(int i = 0; i < len_evidence; i++) {
        Cj += beta[i+2]*test_dataset[base_pos + ADD_BASE_@indexer@(row, evidence_index[i], leading_dimension)];
    }

    double diff = (Cj - test_dataset[base_pos + ADD_BASE_@indexer@(row, variable_index, leading_dimension)]);
    s1[row] += beta[1]*diff*inv_variance;
    s3[row] += diff*diff*inv_variance;
}

__kernel void s1_and_s3_sum_constant_@mode@(__constant double* test_dataset,
                                        __private uint leading_dimension,
                                        __constant double* beta,
                                        __private uint variable_index,
                                        __private double inv_variance,
                                        __global double* s1,
                                        __global double* s3
)
{
    uint row = get_global_id(0);
    double Cj = beta[0];

    double diff = (Cj - test_dataset[@indexer@(row, variable_index, leading_dimension)]);
    s1[row] += beta[1]*diff*inv_variance;
    s3[row] += diff*diff*inv_variance;
}

/**end repeat**/

/**begin repeat
 * #train_mode = rowmajor, columnmajor#
 * #indexer = RM, CM#
 */
__kernel void onlygaussian_exponent_coefficients_iterate_test_@train_mode@(__global double* training_dataset,
                                             __private uint train_leading_dimension,
                                             __global double* precision,
                                             __global double* s1,
                                             __private double inv_a,
                                             __global double* s3,
                                             __private uint test_index,
                                             __global double* train_coefficients
                                             )
{
    int i = get_global_id(0);

    double precisionK = precision[0];
    double instanceK = training_dataset[BASE_@indexer@(i, 0, train_leading_dimension)];

    double diff_numerator = instanceK*precisionK - s1[test_index];
    train_coefficients[i] = diff_numerator*diff_numerator*inv_a - 0.5*instanceK*instanceK*precisionK - 0.5*s3[test_index];
}

__kernel void onlygaussian_exponent_coefficients_iterate_train_high_memory_@train_mode@(__global double* training_dataset,
                                                                        __private uint train_leading_dimension,
                                                                        __global double* precision,
                                                                        __global double* s1,
                                                                        __private double inv_a,
                                                                        __global double* s3,
                                                                        __global double* train_coefficients,
                                                                        __private uint n
)
{
    int i = get_global_id(0);

    int test_index = i / n;
    int train_index = i % n;

    double precisionK = precision[0];
    double instanceK = training_dataset[BASE_@indexer@(train_index, 0, train_leading_dimension)];
    double diff_numerator = instanceK*precisionK - s1[test_index];

    train_coefficients[RM(test_index, train_index, n)] =
            diff_numerator*diff_numerator*inv_a - 0.5*instanceK*instanceK*precisionK - 0.5*s3[test_index];
}


__kernel void onlygaussian_exponent_coefficients_iterate_train_low_memory_checkmax_@train_mode@(__global double* training_dataset,
                                                                __private uint train_leading_dimension,
                                                                __global double* precision,
                                                                __global double* s1,
                                                                __private double inv_a,
                                                                __global double* s3,
                                                                __private uint train_index,
                                                                __global double* max_array
)
{
    int i = get_global_id(0);

    double precisionK = precision[0];
    double instanceK = training_dataset[BASE_@indexer@(train_index, 0, train_leading_dimension)];
    double diff_numerator = instanceK*precisionK - s1[i];

    double coeff = diff_numerator*diff_numerator*inv_a - 0.5*instanceK*instanceK*precisionK - 0.5*s3[i];

    max_array[i] = max(max_array[i], coeff);
}

__kernel void onlygaussian_exponent_coefficients_iterate_train_low_memory_compute_@train_mode@(__global double* training_dataset,
                                                                __private uint train_leading_dimension,
                                                                __global double* precision,
                                                                __global double* s1,
                                                                __private double inv_a,
                                                                __global double* s3,
                                                                __private uint train_index,
                                                                __global double* max_array,
                                                                __global double* final_result
)
{
    int i = get_global_id(0);

    double precisionK = precision[0];
    double instanceK = training_dataset[BASE_@indexer@(train_index, 0, train_leading_dimension)];
    double diff_numerator = instanceK*precisionK - s1[i];

    final_result[i] += exp(diff_numerator*diff_numerator*inv_a - 0.5*instanceK*instanceK*precisionK - 0.5*s3[i]
                    - max_array[i]);
}

/**end repeat**/


/**
##########################################
############ Denominator Mix #############
##########################################
*/

/**begin repeat
 * #train_mode = rowmajor, columnmajor#
 * #train_indexer = RM, CM#
 */

/**begin repeat1
 * #test_mode = rowmajor, columnmajor#
 * #test_indexer = RM, CM#
 */

__kernel void substract_without_origin_from_indices_iterate_test_@train_mode@_@test_mode@(__constant double *train_data,
                                                            __private uint train_leading_dimension,
                                                            __constant double *test_data,
                                                            __private uint test_leading_dimension,
                                                            __global double *res,
                                                            __private uint test_row,
                                                            __private uint nparents_kde,
                                                            __constant uint* kde_indices) {

    int gid = get_global_id(0);

    int r = gid / nparents_kde;
    int c = gid % nparents_kde;

    res[RM(r, c, nparents_kde)] = test_data[@test_indexer@(test_row, kde_indices[c], test_leading_dimension)]
                            - train_data[@train_indexer@(r, c+1, train_leading_dimension)];
}

/**end repeat1**/

/**end repeat**/

/**begin repeat
 * #train_mode = rowmajor, columnmajor#
 * #train_indexer = RM, CM#
 */

/**begin repeat1
 * #test_mode = rowmajor, columnmajor#
 * #test_indexer = RM, CM#
 */

__kernel void substract_without_origin_from_indices_iterate_train_@train_mode@_@test_mode@(__constant double *train_data,
        __private uint train_leading_dimension,
        __constant double *test_data,
        __private uint test_leading_dimension,
        __global double *res,
        __private uint train_row,
        __private uint nparents_kde,
        __constant uint* kde_indices) {

    int gid = get_global_id(0);

    int r = gid / nparents_kde;
    int c = gid % nparents_kde;

    res[RM(r, c, nparents_kde)] = test_data[@test_indexer@(r, kde_indices[c], test_leading_dimension)]
                                - train_data[@train_indexer@(train_row, c+1, train_leading_dimension)];
}

/**end repeat1**/

/**end repeat**/


__kernel void mahalanobis(__constant double *Ti,
                        __constant double* precision,
                        __global double* res,
                        __local double* sums_buffer,
                        __private uint nparents_kde
)
{
    uint gid = get_global_id(0);
    uint lid = get_local_id(0);

    uint instance = get_group_id(0);

    uint p = lid / nparents_kde;
    uint q = lid % nparents_kde;

    sums_buffer[lid] = Ti[RM(instance, p, nparents_kde)]*Ti[RM(instance, q, nparents_kde)]*
                        precision[RM(p+1,q+1,nparents_kde+1)];

    uint remaining_sum = nparents_kde*nparents_kde;

    while (remaining_sum > 1) {
        uint stride = remaining_sum / 2;
        barrier(CLK_LOCAL_MEM_FENCE);

            if (lid < stride) {
                sums_buffer[lid] += sums_buffer[lid+stride];
            }

            if (remaining_sum % 2 != 0 && lid == 0) {
                sums_buffer[lid] += sums_buffer[lid+remaining_sum-1];
            }

            remaining_sum = stride;
    }

    if (lid == 0) {
        res[instance] = sums_buffer[0];
    }
}

__kernel void mahalanobis_mat(__constant double *Ti,
        __constant double* precision,
        __global double* res,
        __local double* sums_buffer,
        __private uint nparents_kde,
        __private uint train_index,
        __private uint n
)
{
    uint gid = get_global_id(0);
    uint lid = get_local_id(0);

    uint instance = get_group_id(0);

    uint p = lid / nparents_kde;
    uint q = lid % nparents_kde;

    sums_buffer[lid] = Ti[RM(instance, p, nparents_kde)]*Ti[RM(instance, q, nparents_kde)]*
    precision[RM(p+1,q+1,nparents_kde+1)];

    uint remaining_sum = nparents_kde*nparents_kde;

    while (remaining_sum > 1) {
        uint stride = remaining_sum / 2;
        barrier(CLK_LOCAL_MEM_FENCE);

        if (lid < stride) {
            sums_buffer[lid] += sums_buffer[lid+stride];
        }

        if (remaining_sum % 2 != 0 && lid == 0) {
            sums_buffer[lid] += sums_buffer[lid+remaining_sum-1];
        }

        remaining_sum = stride;
    }

    if (lid == 0) {
        res[RM(instance, train_index, n)] = sums_buffer[0];
    }
}


__kernel void dotproduct(__constant double *Ti,
            __constant double* precision,
            __global double* res,
            __local double* sums_buffer,
            __private uint nparents_kde
)
{
    uint gid = get_global_id(0);
    uint lid = get_local_id(0);

    uint instance = get_group_id(0);


    sums_buffer[lid] = Ti[RM(instance, lid, nparents_kde)]*precision[lid+1];


    uint remaining_sum = nparents_kde;

    while (remaining_sum > 1) {
        uint stride = remaining_sum / 2;
        barrier(CLK_LOCAL_MEM_FENCE);

        if (lid < stride) {
            sums_buffer[lid] += sums_buffer[lid+stride];
        }

        if (remaining_sum % 2 != 0 && lid == 0) {
            sums_buffer[lid] += sums_buffer[lid+remaining_sum-1];
        }

        remaining_sum = stride;
    }

    if (lid == 0) {
        res[instance] = sums_buffer[0];
    }
}

/**begin repeat
 * #train_mode = rowmajor, columnmajor#
 * #train_indexer = RM, CM#
 */

__kernel void exponent_coefficients_iterate_test_@train_mode@(__constant double *train_data,
                                                __private uint train_leading_dimension,
                                                __constant double* precision,
                                                __global double* mahalanobis,
                                                __constant double* dotproduct,
                                                __constant double* s1,
                                                __private double inv_a,
                                                __constant double* s3,
                                                __private uint test_index

)
{
    uint gid = get_global_id(0);

    double train_variable = train_data[BASE_@train_indexer@(gid, 0, train_leading_dimension)];
    double dot_instance = dotproduct[gid];

    double bi = train_variable*precision[0] - dot_instance - s1[test_index];

    double ci = 0.5*(mahalanobis[gid] - 2*train_variable*dot_instance
            + train_variable*train_variable*precision[0] + s3[test_index]);

    mahalanobis[gid] = bi*bi*inv_a - ci;
}

/**end repeat**/

/**begin repeat
 * #train_mode = rowmajor, columnmajor#
 * #train_indexer = RM, CM#
 */

__kernel void exponent_coefficients_iterate_train_high_memory_@train_mode@(__constant double *train_data,
                                        __private uint train_leading_dimension,
                                        __constant double* precision,
                                        __global double* coeffs,
                                        __constant double* dotproduct,
                                        __constant double* s1,
                                        __private double inv_a,
                                        __constant double* s3,
                                        __private uint train_index,
                                        __private uint n
)
{
    uint gid = get_global_id(0);

    double train_variable = train_data[BASE_@train_indexer@(train_index, 0, train_leading_dimension)];
//    Negate the dotproduct because we computed Train - Test in substract instead of Test - Train.
    double dot_instance = dotproduct[gid];

    double bi = train_variable*precision[0] - dot_instance - s1[gid];


    double ci = 0.5*(coeffs[RM(gid, train_index, n)] - 2*train_variable*dot_instance
                 + train_variable*train_variable*precision[0] + s3[gid]);

//    printf("gid %d, train_index %d, bi %f, ci %f, dotproduct %f",
//            gid, train_index, bi, ci, dot_instance);

    coeffs[RM(gid, train_index, n)] = bi*bi*inv_a - ci;
}

/**end repeat**/

/**begin repeat
 * #train_mode = rowmajor, columnmajor#
 * #train_indexer = RM, CM#
 */

__kernel void exponent_coefficients_iterate_train_low_memory_checkmax_@train_mode@(__constant double *train_data,
                                        __private uint train_leading_dimension,
                                        __constant double* precision,
                                        __constant double* mahalanobis,
                                        __global double* max_coeffs,
                                        __constant double* dotproduct,
                                        __constant double* s1,
                                        __private double inv_a,
                                        __constant double* s3,
                                        __private uint train_index
)
{
    uint gid = get_global_id(0);

    double train_variable = train_data[BASE_@train_indexer@(train_index, 0, train_leading_dimension)];
    double dot_instance = dotproduct[gid];

    double bi = train_variable*precision[0] - dot_instance - s1[gid];

    double ci = 0.5*(mahalanobis[gid] - 2*train_variable*dot_instance
                     + train_variable*train_variable*precision[0] + s3[gid]);

    max_coeffs[gid] = max(max_coeffs[gid], bi*bi*inv_a - ci);
}

/**end repeat**/

/**begin repeat
 * #train_mode = rowmajor, columnmajor#
 * #train_indexer = RM, CM#
 */

__kernel void exponent_coefficients_iterate_train_low_memory_compute_@train_mode@(__constant double *train_data,
                                        __private uint train_leading_dimension,
                                        __constant double* precision,
                                        __constant double* mahalanobis,
                                        __global double* coeffs,
                                        __constant double* max_coeffs,
                                        __constant double* dotproduct,
                                        __constant double* s1,
                                        __private double inv_a,
                                        __constant double* s3,
                                        __private uint train_index
)
{
    uint gid = get_global_id(0);

    double train_variable = train_data[BASE_@train_indexer@(train_index, 0, train_leading_dimension)];
    double dot_instance = dotproduct[gid];

    double bi = train_variable*precision[0] - dot_instance - s1[gid];

    double ci = 0.5*(mahalanobis[gid] - 2*train_variable*dot_instance
                     + train_variable*train_variable*precision[0] + s3[gid]);

    coeffs[gid] += exp(bi*bi*inv_a - ci - max_coeffs[gid]);
}

/**end repeat**/

// Don't let /**end repeat**/ to be the last line: conv_template.py won't work.