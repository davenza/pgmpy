pub const OPEN_CL_CODE: &str = r#"#line 1 "./pgmpy/rust/src/kde_gaussian.cl.src"

/*
 *****************************************************************************
 **       This file was autogenerated from a template  DO NOT EDIT!!!!      **
 **       Changes should be made to the original source (.src) file         **
 *****************************************************************************
 */

#line 1
// Row major indexing for row i column j and leading dimension for the columns
#define RM(i, j, leading) ((i)*(leading) + (j))
// Column major indexing for row i column j and leading dimension for the rows
#define CM(i, j, leading) ((j)*(leading) + (i))

#define BASE_RM(i, j, leading) ((i)*(leading))
#define BASE_CM(i, j, leading) (i)

#define ADD_BASE_RM(i, j, leading) (j)
#define ADD_BASE_CM(i, j, leading) ((j)*(leading))

/**
##########################################
################  MISC  ##################
##########################################
*/


__kernel void fill_value(__global double *vec, __private double value) {
    vec[get_global_id(0)] = value;
}

__kernel void fill_value_uint(__global uint *vec, __private uint value) {
    vec[get_global_id(0)] = value;
}

__kernel void sum_vectors(__global double *left, __constant double *right) {
    uint idx = get_global_id(0);
    left[idx] += right[idx];
}

__kernel void sum_constant(__global double *v, __private double c) {
    v[get_global_id(0)] += c;
}


/**
##########################################
###############  COMMON  #################
##########################################
 */

#line 49

#line 54

__kernel void substract_rowmajor_rowmajor(__constant double *train_data,
                                            __private uint train_cols,
                                            __constant double *vec,
                                            __global double *res,
                                            __private uint row,
                                            __private uint train_leading_dimension,
                                            __private uint test_leading_dimension)
{
    int i = get_global_id(0);

    int r = i / train_leading_dimension;
    int c = i % train_leading_dimension;

    res[RM(r, c, train_cols)] = train_data[i] - vec[RM(row, c, test_leading_dimension)];
}


#line 54

__kernel void substract_rowmajor_columnmajor(__constant double *train_data,
                                            __private uint train_cols,
                                            __constant double *vec,
                                            __global double *res,
                                            __private uint row,
                                            __private uint train_leading_dimension,
                                            __private uint test_leading_dimension)
{
    int i = get_global_id(0);

    int r = i / train_leading_dimension;
    int c = i % train_leading_dimension;

    res[RM(r, c, train_cols)] = train_data[i] - vec[CM(row, c, test_leading_dimension)];
}




#line 49

#line 54

__kernel void substract_columnmajor_rowmajor(__constant double *train_data,
                                            __private uint train_cols,
                                            __constant double *vec,
                                            __global double *res,
                                            __private uint row,
                                            __private uint train_leading_dimension,
                                            __private uint test_leading_dimension)
{
    int i = get_global_id(0);

    int r = i % train_leading_dimension;
    int c = i / train_leading_dimension;

    res[RM(r, c, train_cols)] = train_data[i] - vec[RM(row, c, test_leading_dimension)];
}


#line 54

__kernel void substract_columnmajor_columnmajor(__constant double *train_data,
                                            __private uint train_cols,
                                            __constant double *vec,
                                            __global double *res,
                                            __private uint row,
                                            __private uint train_leading_dimension,
                                            __private uint test_leading_dimension)
{
    int i = get_global_id(0);

    int r = i % train_leading_dimension;
    int c = i / train_leading_dimension;

    res[RM(r, c, train_cols)] = train_data[i] - vec[CM(row, c, test_leading_dimension)];
}





__kernel void solve(__global double *diff_data, __constant double *chol, __private uint n_col) {
    uint r = get_global_id(0);
    uint index_row = r * n_col;

    for (uint c = 0; c < n_col; c++) {
        for (uint i = 0; i < c; i++) {
            diff_data[index_row + c] -= chol[c * n_col + i] * diff_data[index_row + i];
        }
        diff_data[index_row + c] /= chol[c * n_col + c];
    }
}

__kernel void square(__global double *solve_data) {
    uint idx = get_global_id(0);
    double d = solve_data[idx];
    solve_data[idx] = d * d;
}

/**
##########################################
#################  PDF  ##################
##########################################
*/

__kernel void pdf_density(__constant double *square_data,
                    __global double *sol_vec,
                    __private uint n_col,
                    __private double lognorm_factor) {
    uint r = get_global_id(0);
    uint idx = r * n_col;

    sol_vec[r] = square_data[idx];
    for (uint i = 1; i < n_col; i++) {
        sol_vec[r] += square_data[idx + i];
    }

    sol_vec[r] = exp(-0.5 * sol_vec[r] - lognorm_factor);
}

__kernel void parallel_sum_gpu(__constant double *input,
                            __private uint n,
                            __local double *localSums,
                            __global double* output) {
    uint global_id = get_global_id(0);
    uint local_id = get_local_id(0);
    uint group_size = get_local_size(0);
    uint group_id = get_group_id(0);
    uint num_groups = get_num_groups(0);

    if (group_id == num_groups-1) {
        group_size = n - group_id*group_size;

        if (global_id < n) {
            localSums[local_id] = input[global_id];
        }
    }
    else {
        localSums[local_id] = input[global_id];
    }

    while (group_size > 1) {
        int stride = group_size / 2;
        barrier(CLK_LOCAL_MEM_FENCE);
        if (group_size % 2 == 0) {
            if (local_id < stride) {
                localSums[local_id] += localSums[local_id + stride];
            }

            group_size = group_size / 2;
        }
        else {
            if (local_id < stride) {
                localSums[local_id+1] += localSums[local_id+1 + stride];
            }
            group_size = (group_size / 2) + 1;
        }
    }

    if (local_id == 0) {
        output[group_id] = localSums[0];
    }
}

__kernel void parallel_sum_gpu_single_wg(__constant double *input,
        __private uint n,
        __local double *localSums,
        __global double* output) {

    uint global_id = get_global_id(0);
    uint group_size = n;

    localSums[global_id] = input[global_id];

    while (group_size > 1) {
        int stride = group_size / 2;
        barrier(CLK_LOCAL_MEM_FENCE);
        if (group_size % 2 == 0) {
            if (global_id < stride) {
                localSums[global_id] += localSums[global_id + stride];
            }

            group_size = group_size / 2;
        }
        else {
            if (global_id < stride) {
                localSums[global_id+1] += localSums[global_id+1 + stride];
            }
            group_size = (group_size / 2) + 1;
        }
    }

    if (global_id == 0) {
        output[0] = localSums[0];
    }
}


/**
##########################################
########  logPDF - Iterate test  #########
##########################################
*/

__kernel void logpdf(__constant double *square_data,
                        __global double *sol_vec,
                        __private uint n_col,
                        __private double lognorm_factor) {
    uint r = get_global_id(0);
    uint idx = r * n_col;

    sol_vec[r] = square_data[idx];
    for (uint i = 1; i < n_col; i++) {
        sol_vec[r] += square_data[idx + i];
    }

    sol_vec[r] = (-0.5 * sol_vec[r]) - lognorm_factor;
}

__kernel void parallel_max_gpu(__constant double *input,
                            __private uint n,
                            __local double *localMaxs,
                            __global double* output)
{
    uint global_id = get_global_id(0);
    uint local_id = get_local_id(0);
    uint group_size = get_local_size(0);
    uint group_id = get_group_id(0);
    uint num_groups = get_num_groups(0);

    if (group_id == num_groups-1) {
        group_size = n - group_id*group_size;

        if (global_id < n) {
            localMaxs[local_id] = input[global_id];
        }
    }
    else {
        localMaxs[local_id] = input[global_id];
    }

    while (group_size > 1) {
        int stride = group_size / 2;
        barrier(CLK_LOCAL_MEM_FENCE);

        if (group_size % 2 == 0) {
            if (local_id < stride) {
                localMaxs[local_id] = max(localMaxs[local_id], localMaxs[local_id + stride]);
            }

            group_size = group_size / 2;
        }
        else {
            if (local_id < stride) {
                localMaxs[local_id+1] = max(localMaxs[local_id+1], localMaxs[local_id+1+stride]);
            }
            group_size = (group_size / 2) + 1;
        }
    }

    if (local_id == 0) {
        output[group_id] = localMaxs[0];
    }
}

__kernel void parallel_max_gpu_single_wg(__constant double *input,
                                        __private uint n,
                                        __local double *localMaxs,
                                        __global double* output)
{
    uint global_id = get_global_id(0);
    uint group_size = n;

    localMaxs[global_id] = input[global_id];

    while (group_size > 1) {
        int stride = group_size / 2;
        barrier(CLK_LOCAL_MEM_FENCE);
        if (group_size % 2 == 0) {
            if (global_id < stride) {
                localMaxs[global_id] = max(localMaxs[global_id], localMaxs[global_id + stride]);
            }

            group_size = group_size / 2;
        }
        else {
            if (global_id < stride) {
                localMaxs[global_id+1] = max(localMaxs[global_id+1], localMaxs[global_id+1 + stride]);
            }
            group_size = (group_size / 2) + 1;
        }
    }

    if (global_id == 0) {
        output[0] = localMaxs[0];
    }
}

__kernel void logsumexp_coeffs(__global double *input,
                               __constant double* max) {
    uint idx = get_global_id(0);
    input[idx] = exp(input[idx] - max[0]);
}

__kernel void copy_logpdf_result(__constant double *logsum,
                                 __constant double *maxexp,
                                 __global double *res,
                                 __private uint res_offset) {
    res[res_offset] = maxexp[0] + log(logsum[0]);
}

/**
##########################################
## logPDF - Iterate train (low memory) ###
##########################################
*/

__kernel void logpdf_checkmax(__constant double *square_data,
                                __global double *max_vec,
                                __private uint n_col,
                                __private double lognorm_factor) {
    uint r = get_global_id(0);
    uint idx = r * n_col;

    double s = square_data[idx];
    for (uint i = 1; i < n_col; i++) {
        s += square_data[idx + i];
    }

    s = (-0.5 * s) - lognorm_factor;
    max_vec[r] = max(max_vec[r], s);
}

__kernel void expmax_and_sum(__constant double* logsum, __constant double* maxexp, __global double *res) {
    uint idx = get_global_id(0);
    res[idx] += exp(logsum[idx] - maxexp[idx]);
}

__kernel void log_and_sum(__global double* res, __constant double* maxexp) {
    uint idx = get_global_id(0);
    res[idx] = log(res[idx]) + maxexp[idx];
}



/**
##########################################
## logPDF - Iterate train (high memory) ##
##########################################
*/

__kernel void logpdf_mat(__constant double *square_data,
                                    __global double *sol_mat,
                                    __private uint n_col,
                                    __private uint sol_row,
                                    __private uint n_train_instances,
                                    __private double lognorm_factor) {
    uint r = n_train_instances*get_global_id(0) + sol_row;
    uint idx = get_global_id(0) * n_col;

    sol_mat[r] = square_data[idx];
    for (uint i = 1; i < n_col; i++) {
        sol_mat[r] += square_data[idx + i];
    }

    sol_mat[r] = (-0.5 * sol_mat[r]) - lognorm_factor;
}

__kernel void parallel_max_mat_gpu(__constant double *input,
                                __private uint rows,
                                __private uint cols,
                                __local double *localMaxs,
                                __global double* output)
{
    uint global_id_row = get_global_id(0);
    uint global_id_col = get_global_id(1);
    uint local_id = get_local_id(1);
    uint group_size = get_local_size(1);
    uint group_id = get_group_id(1);
    uint num_groups = get_num_groups(1);

    if (group_id == num_groups-1) {
        group_size = cols - group_id*group_size;

        if (global_id_col < cols) {
            localMaxs[local_id] = input[RM(global_id_row, global_id_col, cols)];
        }
    }
    else {
        localMaxs[local_id] = input[RM(global_id_row, global_id_col, cols)];
    }

    while (group_size > 1) {
        int stride = group_size / 2;
        barrier(CLK_LOCAL_MEM_FENCE);
        if (group_size % 2 == 0) {
            if (local_id < stride) {
                localMaxs[local_id] = max(localMaxs[local_id], localMaxs[local_id + stride]);
            }

            group_size = group_size / 2;
        }
        else {
            if (local_id < stride) {
                localMaxs[local_id+1] = max(localMaxs[local_id+1], localMaxs[local_id+1+stride]);
            }

            group_size = (group_size / 2) + 1;
        }
    }

    if (local_id == 0) {
        output[RM(global_id_row, group_id, num_groups)] = localMaxs[0];
    }
}

__kernel void parallel_max_mat_gpu_single_wg(__constant double *input,
        __private uint rows,
        __private uint cols,
        __local double *localMaxs,
        __global double* output)
{
    uint global_id_row = get_global_id(0);
    uint global_id_col = get_global_id(1);

    uint group_size = cols;

    localMaxs[global_id_col] = input[RM(global_id_row, global_id_col, cols)];

    while (group_size > 1) {
        int stride = group_size / 2;
        barrier(CLK_LOCAL_MEM_FENCE);
        if (group_size % 2 == 0) {
            if (global_id_col < stride) {
                localMaxs[global_id_col] = max(localMaxs[global_id_col],
                                                localMaxs[global_id_col + stride]);
            }

            group_size = group_size / 2;
        }
        else {
            if (global_id_col < stride) {
                localMaxs[global_id_col+1] = max(localMaxs[global_id_col+1],
                                                localMaxs[global_id_col+1+stride]);
            }

            group_size = (group_size / 2) + 1;
        }
    }

    if (global_id_col == 0) {
        output[global_id_row] = localMaxs[0];
    }
}

__kernel void parallel_sum_mat_gpu(__constant double *input,
        __private uint rows,
        __private uint cols,
        __local double *localSums,
        __global double* output)
{
    uint global_id_row = get_global_id(0);
    uint global_id_col = get_global_id(1);
    uint local_id = get_local_id(1);
    uint group_size = get_local_size(1);
    uint group_id = get_group_id(1);
    uint num_groups = get_num_groups(1);

    if (group_id == num_groups-1) {
        group_size = cols - group_id*group_size;

        if (global_id_col < cols) {
            localSums[local_id] = input[RM(global_id_row, global_id_col, cols)];
        }
    }
    else {
        localSums[local_id] = input[RM(global_id_row, global_id_col, cols)];
    }

    while (group_size > 1) {
        int stride = group_size / 2;
        barrier(CLK_LOCAL_MEM_FENCE);
        if (group_size % 2 == 0) {
            if (local_id < stride) {
                localSums[local_id] += localSums[local_id + stride];
            }

            group_size = group_size / 2;
        }
        else {
            if (local_id < stride) {
                localSums[local_id+1] += localSums[local_id+1+stride];
            }

            group_size = (group_size / 2) + 1;
        }
    }

    if (local_id == 0) {
        output[RM(global_id_row, group_id, num_groups)] = localSums[0];
    }
}


__kernel void parallel_sum_mat_gpu_single_wg(__constant double *input,
        __private uint rows,
        __private uint cols,
        __local double *localSums,
        __global double* output,
        __private uint output_leading
)
{
    uint global_id_row = get_global_id(0);
    uint global_id_col = get_global_id(1);

    uint group_size = cols;

    localSums[global_id_col] = input[RM(global_id_row, global_id_col, cols)];

    while (group_size > 1) {
        int stride = group_size / 2;
        barrier(CLK_LOCAL_MEM_FENCE);
        if (group_size % 2 == 0) {
            if (global_id_col < stride) {
                localSums[global_id_col] += localSums[global_id_col + stride];
            }

            group_size = group_size / 2;
        }
        else {
            if (global_id_col < stride) {
                localSums[global_id_col+1] += localSums[global_id_col+1+stride];
            }

            group_size = (group_size / 2) + 1;
        }
    }

    if (global_id_col == 0) {
        output[RM(global_id_row, 0, output_leading)] = localSums[0];
    }
}

__kernel void expmax_mat(__global double* res,
                            __constant double* max_buffer,
                            __private uint n_cols) {
    uint idx = get_global_id(0);
    uint row = idx / n_cols;

    res[idx] = exp(res[idx] - max_buffer[row]);
}

__kernel void log_and_sum_mat(__global double* output,
                                __constant double *summed_mat,
                                __constant double* maxexp,
                                __private uint leading_summed
) {
    uint idx = get_global_id(0);
    output[idx] = log(summed_mat[idx*leading_summed]) + maxexp[idx];
}

/**
##########################################
########## Denominator Only KDE ##########
##########################################
*/

#line 565

#line 570

__kernel void substract_without_origin_rowmajor_rowmajor(__constant double *train_data,
                                                                __private uint train_leading_dimension,
                                                                __constant double *test_data,
                                                                __private uint test_leading_dimension,
                                                                __global double *res,
                                                                __private uint test_row,
                                                                __private uint n_cols) {

    int gid = get_global_id(0);

    int r = gid / n_cols;
    int c = gid % n_cols;

    res[RM(r, c, n_cols)] = test_data[RM(test_row, c+1, test_leading_dimension)]
                        - train_data[RM(r, c+1, train_leading_dimension)];
}


#line 570

__kernel void substract_without_origin_rowmajor_columnmajor(__constant double *train_data,
                                                                __private uint train_leading_dimension,
                                                                __constant double *test_data,
                                                                __private uint test_leading_dimension,
                                                                __global double *res,
                                                                __private uint test_row,
                                                                __private uint n_cols) {

    int gid = get_global_id(0);

    int r = gid / n_cols;
    int c = gid % n_cols;

    res[RM(r, c, n_cols)] = test_data[CM(test_row, c+1, test_leading_dimension)]
                        - train_data[RM(r, c+1, train_leading_dimension)];
}




#line 565

#line 570

__kernel void substract_without_origin_columnmajor_rowmajor(__constant double *train_data,
                                                                __private uint train_leading_dimension,
                                                                __constant double *test_data,
                                                                __private uint test_leading_dimension,
                                                                __global double *res,
                                                                __private uint test_row,
                                                                __private uint n_cols) {

    int gid = get_global_id(0);

    int r = gid / n_cols;
    int c = gid % n_cols;

    res[RM(r, c, n_cols)] = test_data[RM(test_row, c+1, test_leading_dimension)]
                        - train_data[CM(r, c+1, train_leading_dimension)];
}


#line 570

__kernel void substract_without_origin_columnmajor_columnmajor(__constant double *train_data,
                                                                __private uint train_leading_dimension,
                                                                __constant double *test_data,
                                                                __private uint test_leading_dimension,
                                                                __global double *res,
                                                                __private uint test_row,
                                                                __private uint n_cols) {

    int gid = get_global_id(0);

    int r = gid / n_cols;
    int c = gid % n_cols;

    res[RM(r, c, n_cols)] = test_data[CM(test_row, c+1, test_leading_dimension)]
                        - train_data[CM(r, c+1, train_leading_dimension)];
}





__kernel void precompute_marginal_precision(__constant double* precision,
                                                        __private double inv_precision_variable,
                                                        __private uint d,
                                                        __global double* res) {
    uint gid = get_global_id(0);

    uint r = gid / (d-1);
    uint c = gid % (d-1);


    res[RM(r,c,d-1)] = precision[r+1]*precision[c+1]*inv_precision_variable - precision[RM(r+1,c+1,d)];
}

__kernel void onlykde_exponent_coefficients_iterate_test(__global double* Ti,
        __private uint nparents_kde,
        __global double* marginal_precision,
        __global double* train_coefficients,
        __local double* sums_buffer
)
{
    uint gid = get_global_id(0);
    uint lid = get_local_id(0);

    uint instance = get_group_id(0);

    uint p = lid / nparents_kde;
    uint q = lid % nparents_kde;

    uint base_pos = BASE_RM(instance, 0, nparents_kde);

    double Tp = Ti[base_pos + ADD_BASE_RM(instance, p, nparents_kde)];
    double Tq = Ti[base_pos + ADD_BASE_RM(instance, q, nparents_kde)];

    sums_buffer[lid] = Tq*marginal_precision[RM(q,p,nparents_kde)];

    uint remaining_sum = nparents_kde;

    if (q < remaining_sum / 2) {
        while (remaining_sum > 1) {
            uint stride = remaining_sum / 2;
            barrier(CLK_LOCAL_MEM_FENCE);

            if (q < stride) {
                sums_buffer[lid] += sums_buffer[lid+stride];
            }

            if (remaining_sum % 2 != 0 && q == 0) {
                sums_buffer[lid] += sums_buffer[lid+remaining_sum-1];
            }

            remaining_sum = stride;
        }
    }

    barrier(CLK_LOCAL_MEM_FENCE);
    if (q == 0) {
        sums_buffer[p] = sums_buffer[lid] * Tp;
    }

    remaining_sum = nparents_kde;

    if (lid < remaining_sum / 2) {
        while (remaining_sum > 1) {
            uint stride = remaining_sum / 2;
            barrier(CLK_LOCAL_MEM_FENCE);

            if (lid < stride) {
                sums_buffer[lid] += sums_buffer[lid+stride];
            }

            if (remaining_sum % 2 != 0 && lid == 0) {
                sums_buffer[lid] += sums_buffer[lid+remaining_sum-1];
            }

            remaining_sum = stride;
        }
    }

    if (lid == 0) {
        train_coefficients[instance] = 0.5*sums_buffer[0];
    }
}

__kernel void onlykde_exponent_coefficients_iterate_train_high_memory(__constant double* Ti,
        __private uint nparents_kde,
        __constant double* marginal_precision,
        __global double* train_coefficients,
        __local double* sums_buffer,
        __private uint train_index,
        __private uint n
)
{
    uint gid = get_global_id(0);
    uint lid = get_local_id(0);

    uint instance = get_group_id(0);

    uint p = lid / nparents_kde;
    uint q = lid % nparents_kde;

    uint base_pos = BASE_RM(instance, 0, nparents_kde);

    double Tp = Ti[base_pos + ADD_BASE_RM(instance, p, nparents_kde)];
    double Tq = Ti[base_pos + ADD_BASE_RM(instance, q, nparents_kde)];

    sums_buffer[lid] = Tq*marginal_precision[RM(q,p,nparents_kde)];

    uint remaining_sum = nparents_kde;

    if (q < remaining_sum / 2) {
        while (remaining_sum > 1) {
            uint stride = remaining_sum / 2;
            barrier(CLK_LOCAL_MEM_FENCE);

            if (q < stride) {
                sums_buffer[lid] += sums_buffer[lid+stride];
            }

            if (remaining_sum % 2 != 0 && q == 0) {
                sums_buffer[lid] += sums_buffer[lid+remaining_sum-1];
            }

            remaining_sum = stride;
        }
    }

    barrier(CLK_LOCAL_MEM_FENCE);
    if (q == 0) {
        sums_buffer[p] = sums_buffer[lid] * Tp;
    }

    remaining_sum = nparents_kde;

    if (lid < remaining_sum / 2) {
        while (remaining_sum > 1) {
            uint stride = remaining_sum / 2;
            barrier(CLK_LOCAL_MEM_FENCE);

            if (lid < stride) {
                sums_buffer[lid] += sums_buffer[lid+stride];
            }

            if (remaining_sum % 2 != 0 && lid == 0) {
                sums_buffer[lid] += sums_buffer[lid+remaining_sum-1];
            }

            remaining_sum = stride;
        }
    }

    if (lid == 0) {
        train_coefficients[RM(instance, train_index, n)] = 0.5*sums_buffer[0];
    }
}

__kernel void onlykde_exponent_coefficients_iterate_train_low_memory_checkmax(__constant double* Ti,
        __private uint nparents_kde,
        __constant double* marginal_precision,
        __global double* max_buffer,
        __local double* sums_buffer
)
{

    uint gid = get_global_id(0);
    uint lid = get_local_id(0);

    uint instance = get_group_id(0);

    uint p = lid / nparents_kde;
    uint q = lid % nparents_kde;

    uint base_pos = BASE_RM(instance, 0, nparents_kde);

    double Tp = Ti[base_pos + ADD_BASE_RM(instance, p, nparents_kde)];
    double Tq = Ti[base_pos + ADD_BASE_RM(instance, q, nparents_kde)];

    sums_buffer[lid] = Tq*marginal_precision[RM(q,p,nparents_kde)];

    uint remaining_sum = nparents_kde;

    if (q < remaining_sum / 2) {
        while (remaining_sum > 1) {
            uint stride = remaining_sum / 2;
            barrier(CLK_LOCAL_MEM_FENCE);

            if (q < stride) {
                sums_buffer[lid] += sums_buffer[lid+stride];
            }

            if (remaining_sum % 2 != 0 && q == 0) {
                sums_buffer[lid] += sums_buffer[lid+remaining_sum-1];
            }

            remaining_sum = stride;
        }
    }

    barrier(CLK_LOCAL_MEM_FENCE);
    if (q == 0) {
        sums_buffer[p] = sums_buffer[lid] * Tp;
    }

    remaining_sum = nparents_kde;

    if (lid < remaining_sum / 2) {
        while (remaining_sum > 1) {
            uint stride = remaining_sum / 2;
            barrier(CLK_LOCAL_MEM_FENCE);

            if (lid < stride) {
                sums_buffer[lid] += sums_buffer[lid+stride];
            }

            if (remaining_sum % 2 != 0 && lid == 0) {
                sums_buffer[lid] += sums_buffer[lid+remaining_sum-1];
            }

            remaining_sum = stride;
        }
    }

    if (lid == 0) {
        max_buffer[instance] = max(max_buffer[instance], 0.5*sums_buffer[0]);
    }
}

__kernel void onlykde_exponent_coefficients_iterate_train_low_memory_compute(__constant double* Ti,
        __private uint nparents_kde,
        __constant double* marginal_precision,
        __global double* final_result,
        __constant double* max_buffer,
        __local double* sums_buffer
)
{

    uint gid = get_global_id(0);
    uint lid = get_local_id(0);

    uint instance = get_group_id(0);

    uint p = lid / nparents_kde;
    uint q = lid % nparents_kde;

    uint base_pos = BASE_RM(instance, 0, nparents_kde);

    double Tp = Ti[base_pos + ADD_BASE_RM(instance, p, nparents_kde)];
    double Tq = Ti[base_pos + ADD_BASE_RM(instance, q, nparents_kde)];

    sums_buffer[lid] = Tq*marginal_precision[RM(q,p,nparents_kde)];

    uint remaining_sum = nparents_kde;

    while (remaining_sum > 1) {
        uint stride = remaining_sum / 2;
        barrier(CLK_LOCAL_MEM_FENCE);

        if (q < stride) {
            sums_buffer[lid] += sums_buffer[lid+stride];
        }

        if (remaining_sum % 2 != 0 && q == 0) {
            sums_buffer[lid] += sums_buffer[lid+remaining_sum-1];
        }

        remaining_sum = stride;
    }

    barrier(CLK_LOCAL_MEM_FENCE);
    if (q == 0) {
        sums_buffer[p] = sums_buffer[lid] * Tp;
    }

    remaining_sum = nparents_kde;

    while (remaining_sum > 1) {
        uint stride = remaining_sum / 2;
        barrier(CLK_LOCAL_MEM_FENCE);

        if (lid<stride) {
            sums_buffer[lid] += sums_buffer[lid+stride];
        }

        if (remaining_sum % 2 != 0 && lid == 0) {
            sums_buffer[lid] += sums_buffer[lid+remaining_sum-1];
        }

        remaining_sum = stride;
    }

    if (lid == 0) {
        final_result[instance] += exp(0.5*sums_buffer[0] - max_buffer[instance]);
    }
}

/**
##########################################
####### Denominator Only Gaussian ########
##########################################
*/

#line 896

__kernel void s1_and_s3_sum_parents_rowmajor(__constant double* test_dataset,
                                        __private uint leading_dimension,
                                        __constant double* beta,
                                        __private uint variable_index,
                                        __constant uint* evidence_index,
                                        __private uint len_evidence,
                                        __private double inv_variance,
                                        __global double* s1,
                                        __global double* s3
)
{
    uint row = get_global_id(0);
    double Cj = beta[0];

    uint base_pos = BASE_RM(row, 0, leading_dimension);
    for(int i = 0; i < len_evidence; i++) {
        Cj += beta[i+2]*test_dataset[base_pos + ADD_BASE_RM(row, evidence_index[i], leading_dimension)];
    }

    double diff = (Cj - test_dataset[base_pos + ADD_BASE_RM(row, variable_index, leading_dimension)]);
    s1[row] += beta[1]*diff*inv_variance;
    s3[row] += diff*diff*inv_variance;
}

__kernel void s1_and_s3_sum_constant_rowmajor(__constant double* test_dataset,
                                        __private uint leading_dimension,
                                        __constant double* beta,
                                        __private uint variable_index,
                                        __private double inv_variance,
                                        __global double* s1,
                                        __global double* s3
)
{
    uint row = get_global_id(0);
    double Cj = beta[0];

    double diff = (Cj - test_dataset[RM(row, variable_index, leading_dimension)]);
    s1[row] += beta[1]*diff*inv_variance;
    s3[row] += diff*diff*inv_variance;
}


#line 896

__kernel void s1_and_s3_sum_parents_columnmajor(__constant double* test_dataset,
                                        __private uint leading_dimension,
                                        __constant double* beta,
                                        __private uint variable_index,
                                        __constant uint* evidence_index,
                                        __private uint len_evidence,
                                        __private double inv_variance,
                                        __global double* s1,
                                        __global double* s3
)
{
    uint row = get_global_id(0);
    double Cj = beta[0];

    uint base_pos = BASE_CM(row, 0, leading_dimension);
    for(int i = 0; i < len_evidence; i++) {
        Cj += beta[i+2]*test_dataset[base_pos + ADD_BASE_CM(row, evidence_index[i], leading_dimension)];
    }

    double diff = (Cj - test_dataset[base_pos + ADD_BASE_CM(row, variable_index, leading_dimension)]);
    s1[row] += beta[1]*diff*inv_variance;
    s3[row] += diff*diff*inv_variance;
}

__kernel void s1_and_s3_sum_constant_columnmajor(__constant double* test_dataset,
                                        __private uint leading_dimension,
                                        __constant double* beta,
                                        __private uint variable_index,
                                        __private double inv_variance,
                                        __global double* s1,
                                        __global double* s3
)
{
    uint row = get_global_id(0);
    double Cj = beta[0];

    double diff = (Cj - test_dataset[CM(row, variable_index, leading_dimension)]);
    s1[row] += beta[1]*diff*inv_variance;
    s3[row] += diff*diff*inv_variance;
}



#line 944
__kernel void onlygaussian_exponent_coefficients_iterate_test_rowmajor(__constant double* training_dataset,
                                             __private uint train_leading_dimension,
                                            __constant double* precision,
                                            __constant double* s1,
                                             __private double inv_a,
                                             __constant double* s3,
                                             __private uint test_index,
                                             __global double* train_coefficients
                                             )
{
    int i = get_global_id(0);

    double precisionK = precision[0];
    double instanceK = training_dataset[BASE_RM(i, 0, train_leading_dimension)];

    double diff_numerator = instanceK*precisionK - s1[test_index];
    train_coefficients[i] = diff_numerator*diff_numerator*inv_a - 0.5*instanceK*instanceK*precisionK - 0.5*s3[test_index];
}

__kernel void onlygaussian_exponent_coefficients_iterate_train_high_memory_rowmajor(__constant double* training_dataset,
                                                                        __private uint train_leading_dimension,
                                                                        __constant double* precision,
                                                                        __constant double* s1,
                                                                        __private double inv_a,
                                                                        __constant double* s3,
                                                                        __global double* train_coefficients,
                                                                        __private uint n
)
{
    int i = get_global_id(0);

    int test_index = i / n;
    int train_index = i % n;

    double precisionK = precision[0];
    double instanceK = training_dataset[BASE_RM(train_index, 0, train_leading_dimension)];
    double diff_numerator = instanceK*precisionK - s1[test_index];

    train_coefficients[RM(test_index, train_index, n)] =
            diff_numerator*diff_numerator*inv_a - 0.5*instanceK*instanceK*precisionK - 0.5*s3[test_index];
}


__kernel void onlygaussian_exponent_coefficients_iterate_train_low_memory_checkmax_rowmajor(__constant double* training_dataset,
                                                                __private uint train_leading_dimension,
                                                                __constant double* precision,
                                                                __constant double* s1,
                                                                __private double inv_a,
                                                                __constant double* s3,
                                                                __private uint train_index,
                                                                __global double* max_array
)
{
    int i = get_global_id(0);

    double precisionK = precision[0];
    double instanceK = training_dataset[BASE_RM(train_index, 0, train_leading_dimension)];
    double diff_numerator = instanceK*precisionK - s1[i];

    double coeff = diff_numerator*diff_numerator*inv_a - 0.5*instanceK*instanceK*precisionK - 0.5*s3[i];

    max_array[i] = max(max_array[i], coeff);
}

__kernel void onlygaussian_exponent_coefficients_iterate_train_low_memory_compute_rowmajor(__constant double* training_dataset,
                                                                __private uint train_leading_dimension,
                                                                __constant double* precision,
                                                                __constant double* s1,
                                                                __private double inv_a,
                                                                __constant double* s3,
                                                                __private uint train_index,
                                                                __constant double* max_array,
                                                                __global double* final_result
)
{
    int i = get_global_id(0);

    double precisionK = precision[0];
    double instanceK = training_dataset[BASE_RM(train_index, 0, train_leading_dimension)];
    double diff_numerator = instanceK*precisionK - s1[i];

    final_result[i] += exp(diff_numerator*diff_numerator*inv_a - 0.5*instanceK*instanceK*precisionK - 0.5*s3[i]
                    - max_array[i]);
}


#line 944
__kernel void onlygaussian_exponent_coefficients_iterate_test_columnmajor(__constant double* training_dataset,
                                             __private uint train_leading_dimension,
                                            __constant double* precision,
                                            __constant double* s1,
                                             __private double inv_a,
                                             __constant double* s3,
                                             __private uint test_index,
                                             __global double* train_coefficients
                                             )
{
    int i = get_global_id(0);

    double precisionK = precision[0];
    double instanceK = training_dataset[BASE_CM(i, 0, train_leading_dimension)];

    double diff_numerator = instanceK*precisionK - s1[test_index];
    train_coefficients[i] = diff_numerator*diff_numerator*inv_a - 0.5*instanceK*instanceK*precisionK - 0.5*s3[test_index];
}

__kernel void onlygaussian_exponent_coefficients_iterate_train_high_memory_columnmajor(__constant double* training_dataset,
                                                                        __private uint train_leading_dimension,
                                                                        __constant double* precision,
                                                                        __constant double* s1,
                                                                        __private double inv_a,
                                                                        __constant double* s3,
                                                                        __global double* train_coefficients,
                                                                        __private uint n
)
{
    int i = get_global_id(0);

    int test_index = i / n;
    int train_index = i % n;

    double precisionK = precision[0];
    double instanceK = training_dataset[BASE_CM(train_index, 0, train_leading_dimension)];
    double diff_numerator = instanceK*precisionK - s1[test_index];

    train_coefficients[RM(test_index, train_index, n)] =
            diff_numerator*diff_numerator*inv_a - 0.5*instanceK*instanceK*precisionK - 0.5*s3[test_index];
}


__kernel void onlygaussian_exponent_coefficients_iterate_train_low_memory_checkmax_columnmajor(__constant double* training_dataset,
                                                                __private uint train_leading_dimension,
                                                                __constant double* precision,
                                                                __constant double* s1,
                                                                __private double inv_a,
                                                                __constant double* s3,
                                                                __private uint train_index,
                                                                __global double* max_array
)
{
    int i = get_global_id(0);

    double precisionK = precision[0];
    double instanceK = training_dataset[BASE_CM(train_index, 0, train_leading_dimension)];
    double diff_numerator = instanceK*precisionK - s1[i];

    double coeff = diff_numerator*diff_numerator*inv_a - 0.5*instanceK*instanceK*precisionK - 0.5*s3[i];

    max_array[i] = max(max_array[i], coeff);
}

__kernel void onlygaussian_exponent_coefficients_iterate_train_low_memory_compute_columnmajor(__constant double* training_dataset,
                                                                __private uint train_leading_dimension,
                                                                __constant double* precision,
                                                                __constant double* s1,
                                                                __private double inv_a,
                                                                __constant double* s3,
                                                                __private uint train_index,
                                                                __constant double* max_array,
                                                                __global double* final_result
)
{
    int i = get_global_id(0);

    double precisionK = precision[0];
    double instanceK = training_dataset[BASE_CM(train_index, 0, train_leading_dimension)];
    double diff_numerator = instanceK*precisionK - s1[i];

    final_result[i] += exp(diff_numerator*diff_numerator*inv_a - 0.5*instanceK*instanceK*precisionK - 0.5*s3[i]
                    - max_array[i]);
}




/**
##########################################
############ Denominator Mix #############
##########################################
*/

#line 1042

#line 1047

__kernel void substract_without_origin_from_indices_iterate_test_rowmajor_rowmajor(__constant double *train_data,
                                                            __private uint train_leading_dimension,
                                                            __constant double *test_data,
                                                            __private uint test_leading_dimension,
                                                            __global double *res,
                                                            __private uint test_row,
                                                            __private uint nparents_kde,
                                                            __constant uint* kde_indices) {

    int gid = get_global_id(0);

    int r = gid / nparents_kde;
    int c = gid % nparents_kde;

    res[RM(r, c, nparents_kde)] = test_data[RM(test_row, kde_indices[c], test_leading_dimension)]
                            - train_data[RM(r, c+1, train_leading_dimension)];
}


#line 1047

__kernel void substract_without_origin_from_indices_iterate_test_rowmajor_columnmajor(__constant double *train_data,
                                                            __private uint train_leading_dimension,
                                                            __constant double *test_data,
                                                            __private uint test_leading_dimension,
                                                            __global double *res,
                                                            __private uint test_row,
                                                            __private uint nparents_kde,
                                                            __constant uint* kde_indices) {

    int gid = get_global_id(0);

    int r = gid / nparents_kde;
    int c = gid % nparents_kde;

    res[RM(r, c, nparents_kde)] = test_data[CM(test_row, kde_indices[c], test_leading_dimension)]
                            - train_data[RM(r, c+1, train_leading_dimension)];
}




#line 1042

#line 1047

__kernel void substract_without_origin_from_indices_iterate_test_columnmajor_rowmajor(__constant double *train_data,
                                                            __private uint train_leading_dimension,
                                                            __constant double *test_data,
                                                            __private uint test_leading_dimension,
                                                            __global double *res,
                                                            __private uint test_row,
                                                            __private uint nparents_kde,
                                                            __constant uint* kde_indices) {

    int gid = get_global_id(0);

    int r = gid / nparents_kde;
    int c = gid % nparents_kde;

    res[RM(r, c, nparents_kde)] = test_data[RM(test_row, kde_indices[c], test_leading_dimension)]
                            - train_data[CM(r, c+1, train_leading_dimension)];
}


#line 1047

__kernel void substract_without_origin_from_indices_iterate_test_columnmajor_columnmajor(__constant double *train_data,
                                                            __private uint train_leading_dimension,
                                                            __constant double *test_data,
                                                            __private uint test_leading_dimension,
                                                            __global double *res,
                                                            __private uint test_row,
                                                            __private uint nparents_kde,
                                                            __constant uint* kde_indices) {

    int gid = get_global_id(0);

    int r = gid / nparents_kde;
    int c = gid % nparents_kde;

    res[RM(r, c, nparents_kde)] = test_data[CM(test_row, kde_indices[c], test_leading_dimension)]
                            - train_data[CM(r, c+1, train_leading_dimension)];
}





#line 1074

#line 1079

__kernel void substract_without_origin_from_indices_iterate_train_rowmajor_rowmajor(__constant double *train_data,
        __private uint train_leading_dimension,
        __constant double *test_data,
        __private uint test_leading_dimension,
        __global double *res,
        __private uint train_row,
        __private uint nparents_kde,
        __constant uint* kde_indices) {

    int gid = get_global_id(0);

    int r = gid / nparents_kde;
    int c = gid % nparents_kde;

    res[RM(r, c, nparents_kde)] = test_data[RM(r, kde_indices[c], test_leading_dimension)]
                                - train_data[RM(train_row, c+1, train_leading_dimension)];
}


#line 1079

__kernel void substract_without_origin_from_indices_iterate_train_rowmajor_columnmajor(__constant double *train_data,
        __private uint train_leading_dimension,
        __constant double *test_data,
        __private uint test_leading_dimension,
        __global double *res,
        __private uint train_row,
        __private uint nparents_kde,
        __constant uint* kde_indices) {

    int gid = get_global_id(0);

    int r = gid / nparents_kde;
    int c = gid % nparents_kde;

    res[RM(r, c, nparents_kde)] = test_data[CM(r, kde_indices[c], test_leading_dimension)]
                                - train_data[RM(train_row, c+1, train_leading_dimension)];
}




#line 1074

#line 1079

__kernel void substract_without_origin_from_indices_iterate_train_columnmajor_rowmajor(__constant double *train_data,
        __private uint train_leading_dimension,
        __constant double *test_data,
        __private uint test_leading_dimension,
        __global double *res,
        __private uint train_row,
        __private uint nparents_kde,
        __constant uint* kde_indices) {

    int gid = get_global_id(0);

    int r = gid / nparents_kde;
    int c = gid % nparents_kde;

    res[RM(r, c, nparents_kde)] = test_data[RM(r, kde_indices[c], test_leading_dimension)]
                                - train_data[CM(train_row, c+1, train_leading_dimension)];
}


#line 1079

__kernel void substract_without_origin_from_indices_iterate_train_columnmajor_columnmajor(__constant double *train_data,
        __private uint train_leading_dimension,
        __constant double *test_data,
        __private uint test_leading_dimension,
        __global double *res,
        __private uint train_row,
        __private uint nparents_kde,
        __constant uint* kde_indices) {

    int gid = get_global_id(0);

    int r = gid / nparents_kde;
    int c = gid % nparents_kde;

    res[RM(r, c, nparents_kde)] = test_data[CM(r, kde_indices[c], test_leading_dimension)]
                                - train_data[CM(train_row, c+1, train_leading_dimension)];
}






__kernel void mahalanobis(__constant double *Ti,
                        __constant double* precision,
                        __global double* res,
                        __local double* sums_buffer,
                        __private uint nparents_kde
)
{
    uint gid = get_global_id(0);
    uint lid = get_local_id(0);

    uint instance = get_group_id(0);

    uint p = lid / nparents_kde;
    uint q = lid % nparents_kde;

    sums_buffer[lid] = Ti[RM(instance, p, nparents_kde)]*Ti[RM(instance, q, nparents_kde)]*
                        precision[RM(p+1,q+1,nparents_kde+1)];

    uint remaining_sum = nparents_kde*nparents_kde;

    while (remaining_sum > 1) {
        uint stride = remaining_sum / 2;
        barrier(CLK_LOCAL_MEM_FENCE);

        if (lid < stride) {
            sums_buffer[lid] += sums_buffer[lid+stride];
        }

        if (remaining_sum % 2 != 0 && lid == 0) {
            sums_buffer[lid] += sums_buffer[lid+remaining_sum-1];
        }

        remaining_sum = stride;
    }

    if (lid == 0) {
        res[instance] = sums_buffer[0];
    }
}

__kernel void mahalanobis_mat(__constant double *Ti,
        __constant double* precision,
        __global double* res,
        __local double* sums_buffer,
        __private uint nparents_kde,
        __private uint train_index,
        __private uint n
)
{
    uint gid = get_global_id(0);
    uint lid = get_local_id(0);

    uint instance = get_group_id(0);

    uint p = lid / nparents_kde;
    uint q = lid % nparents_kde;

    sums_buffer[lid] = Ti[RM(instance, p, nparents_kde)]*Ti[RM(instance, q, nparents_kde)]*
    precision[RM(p+1,q+1,nparents_kde+1)];

    uint remaining_sum = nparents_kde*nparents_kde;

    while (remaining_sum > 1) {
        uint stride = remaining_sum / 2;
        barrier(CLK_LOCAL_MEM_FENCE);

        if (lid < stride) {
            sums_buffer[lid] += sums_buffer[lid+stride];
        }

        if (remaining_sum % 2 != 0 && lid == 0) {
            sums_buffer[lid] += sums_buffer[lid+remaining_sum-1];
        }

        remaining_sum = stride;
    }

    if (lid == 0) {
        res[RM(instance, train_index, n)] = sums_buffer[0];
    }
}


__kernel void dotproduct(__constant double *Ti,
            __constant double* precision,
            __global double* res,
            __local double* sums_buffer,
            __private uint nparents_kde
)
{
    uint gid = get_global_id(0);
    uint lid = get_local_id(0);

    uint instance = get_group_id(0);

    sums_buffer[lid] = Ti[RM(instance, lid, nparents_kde)]*precision[lid+1];

    uint remaining_sum = nparents_kde;

    while (remaining_sum > 1) {
        uint stride = remaining_sum / 2;
        barrier(CLK_LOCAL_MEM_FENCE);

        if (lid < stride) {
            sums_buffer[lid] += sums_buffer[lid+stride];
        }

        if (remaining_sum % 2 != 0 && lid == 0) {
            sums_buffer[lid] += sums_buffer[lid+remaining_sum-1];
        }

        remaining_sum = stride;
    }

    if (lid == 0) {
        res[instance] = sums_buffer[0];
    }
}

#line 1226

__kernel void exponent_coefficients_iterate_test_rowmajor(__constant double *train_data,
                                                __private uint train_leading_dimension,
                                                __constant double* precision,
                                                __global double* mahalanobis,
                                                __constant double* dotproduct,
                                                __constant double* s1,
                                                __private double inv_a,
                                                __constant double* s3,
                                                __private uint test_index

)
{
    uint gid = get_global_id(0);

    double train_variable = train_data[BASE_RM(gid, 0, train_leading_dimension)];
    double dot_instance = dotproduct[gid];

    double bi = train_variable*precision[0] - dot_instance - s1[test_index];

    double ci = 0.5*(mahalanobis[gid] - 2*train_variable*dot_instance
            + train_variable*train_variable*precision[0] + s3[test_index]);

    mahalanobis[gid] = bi*bi*inv_a - ci;
}


#line 1226

__kernel void exponent_coefficients_iterate_test_columnmajor(__constant double *train_data,
                                                __private uint train_leading_dimension,
                                                __constant double* precision,
                                                __global double* mahalanobis,
                                                __constant double* dotproduct,
                                                __constant double* s1,
                                                __private double inv_a,
                                                __constant double* s3,
                                                __private uint test_index

)
{
    uint gid = get_global_id(0);

    double train_variable = train_data[BASE_CM(gid, 0, train_leading_dimension)];
    double dot_instance = dotproduct[gid];

    double bi = train_variable*precision[0] - dot_instance - s1[test_index];

    double ci = 0.5*(mahalanobis[gid] - 2*train_variable*dot_instance
            + train_variable*train_variable*precision[0] + s3[test_index]);

    mahalanobis[gid] = bi*bi*inv_a - ci;
}



#line 1258

__kernel void exponent_coefficients_iterate_train_high_memory_rowmajor(__constant double *train_data,
                                        __private uint train_leading_dimension,
                                        __constant double* precision,
                                        __global double* coeffs,
                                        __constant double* dotproduct,
                                        __constant double* s1,
                                        __private double inv_a,
                                        __constant double* s3,
                                        __private uint train_index,
                                        __private uint n
)
{
    uint gid = get_global_id(0);

    double train_variable = train_data[BASE_RM(train_index, 0, train_leading_dimension)];
//    Negate the dotproduct because we computed Train - Test in substract instead of Test - Train.
    double dot_instance = dotproduct[gid];

    double bi = train_variable*precision[0] - dot_instance - s1[gid];


    double ci = 0.5*(coeffs[RM(gid, train_index, n)] - 2*train_variable*dot_instance
                 + train_variable*train_variable*precision[0] + s3[gid]);

    coeffs[RM(gid, train_index, n)] = bi*bi*inv_a - ci;
}


#line 1258

__kernel void exponent_coefficients_iterate_train_high_memory_columnmajor(__constant double *train_data,
                                        __private uint train_leading_dimension,
                                        __constant double* precision,
                                        __global double* coeffs,
                                        __constant double* dotproduct,
                                        __constant double* s1,
                                        __private double inv_a,
                                        __constant double* s3,
                                        __private uint train_index,
                                        __private uint n
)
{
    uint gid = get_global_id(0);

    double train_variable = train_data[BASE_CM(train_index, 0, train_leading_dimension)];
//    Negate the dotproduct because we computed Train - Test in substract instead of Test - Train.
    double dot_instance = dotproduct[gid];

    double bi = train_variable*precision[0] - dot_instance - s1[gid];


    double ci = 0.5*(coeffs[RM(gid, train_index, n)] - 2*train_variable*dot_instance
                 + train_variable*train_variable*precision[0] + s3[gid]);

    coeffs[RM(gid, train_index, n)] = bi*bi*inv_a - ci;
}



#line 1292

__kernel void exponent_coefficients_iterate_train_low_memory_checkmax_rowmajor(__constant double *train_data,
                                        __private uint train_leading_dimension,
                                        __constant double* precision,
                                        __constant double* mahalanobis,
                                        __global double* max_coeffs,
                                        __constant double* dotproduct,
                                        __constant double* s1,
                                        __private double inv_a,
                                        __constant double* s3,
                                        __private uint train_index
)
{
    uint gid = get_global_id(0);

    double train_variable = train_data[BASE_RM(train_index, 0, train_leading_dimension)];
    double dot_instance = dotproduct[gid];

    double bi = train_variable*precision[0] - dot_instance - s1[gid];

    double ci = 0.5*(mahalanobis[gid] - 2*train_variable*dot_instance
                     + train_variable*train_variable*precision[0] + s3[gid]);

    max_coeffs[gid] = max(max_coeffs[gid], bi*bi*inv_a - ci);
}


#line 1292

__kernel void exponent_coefficients_iterate_train_low_memory_checkmax_columnmajor(__constant double *train_data,
                                        __private uint train_leading_dimension,
                                        __constant double* precision,
                                        __constant double* mahalanobis,
                                        __global double* max_coeffs,
                                        __constant double* dotproduct,
                                        __constant double* s1,
                                        __private double inv_a,
                                        __constant double* s3,
                                        __private uint train_index
)
{
    uint gid = get_global_id(0);

    double train_variable = train_data[BASE_CM(train_index, 0, train_leading_dimension)];
    double dot_instance = dotproduct[gid];

    double bi = train_variable*precision[0] - dot_instance - s1[gid];

    double ci = 0.5*(mahalanobis[gid] - 2*train_variable*dot_instance
                     + train_variable*train_variable*precision[0] + s3[gid]);

    max_coeffs[gid] = max(max_coeffs[gid], bi*bi*inv_a - ci);
}



#line 1324

__kernel void exponent_coefficients_iterate_train_low_memory_compute_rowmajor(__constant double *train_data,
                                        __private uint train_leading_dimension,
                                        __constant double* precision,
                                        __constant double* mahalanobis,
                                        __global double* coeffs,
                                        __constant double* max_coeffs,
                                        __constant double* dotproduct,
                                        __constant double* s1,
                                        __private double inv_a,
                                        __constant double* s3,
                                        __private uint train_index
)
{
    uint gid = get_global_id(0);

    double train_variable = train_data[BASE_RM(train_index, 0, train_leading_dimension)];
    double dot_instance = dotproduct[gid];

    double bi = train_variable*precision[0] - dot_instance - s1[gid];

    double ci = 0.5*(mahalanobis[gid] - 2*train_variable*dot_instance
                     + train_variable*train_variable*precision[0] + s3[gid]);

    coeffs[gid] += exp(bi*bi*inv_a - ci - max_coeffs[gid]);
}


#line 1324

__kernel void exponent_coefficients_iterate_train_low_memory_compute_columnmajor(__constant double *train_data,
                                        __private uint train_leading_dimension,
                                        __constant double* precision,
                                        __constant double* mahalanobis,
                                        __global double* coeffs,
                                        __constant double* max_coeffs,
                                        __constant double* dotproduct,
                                        __constant double* s1,
                                        __private double inv_a,
                                        __constant double* s3,
                                        __private uint train_index
)
{
    uint gid = get_global_id(0);

    double train_variable = train_data[BASE_CM(train_index, 0, train_leading_dimension)];
    double dot_instance = dotproduct[gid];

    double bi = train_variable*precision[0] - dot_instance - s1[gid];

    double ci = 0.5*(mahalanobis[gid] - 2*train_variable*dot_instance
                     + train_variable*train_variable*precision[0] + s3[gid]);

    coeffs[gid] += exp(bi*bi*inv_a - ci - max_coeffs[gid]);
}



// Don't let /**end repeat**/ to be the last line: conv_template.py won't work.
"#;